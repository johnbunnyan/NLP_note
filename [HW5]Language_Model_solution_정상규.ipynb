{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbunnyan/NLP_note/blob/master/%5BHW5%5DLanguage_Model_solution_%EC%A0%95%EC%83%81%EA%B7%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR26RFkwXtvi"
      },
      "source": [
        "# **[HW5] Language Model**\n",
        "1. DataLoader\n",
        "2. Model\n",
        "3. Trainer\n",
        "4. Generation\n",
        "\n",
        "이번 실습에서는 RNN기반의 Language Model를 구현해서 텍스트를 직접 생성해보는 실습을 진행해보겠습니다.\n",
        "\n",
        "- dataset: WikiText2 (https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2)\n",
        "- model: LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crVJ36mMlaXP"
      },
      "source": [
        "\n",
        "\n",
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpvlE_XOWS33"
      },
      "source": [
        "런타임의 유형을 변경해줍니다.\n",
        "\n",
        "상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n",
        "\n",
        "변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqVdEuPQzMAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e84c2f3-fe0d-42f2-aaf1-518d71f2b855"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision #안씀\n",
        "import torch.optim as optim #옵티마이저 - 아담쓸 수 있음\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3-HPdHLZma"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import tqdm #진행률 프로세스 바\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "# 엔비디아 CUDA 딥 뉴럴 네트워크 라이브러리\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GnKJCB4T_Q"
      },
      "source": [
        "# 1. DataLoader\n",
        "\n",
        "이전의 실습들에서 사용한것과 마찬가지로, PyTorch style의 dataloader를 먼저 만들어 두겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcNl0aWbS0OA"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "저희가 이번 실습에서 사용할 데이터셋은 Wikipedia에 있는 영문 글들을 가져온 WikiTree dataset입니다.\n",
        "저희가 불러올 데이터는 가장 작은 WikiTree dataset에서 자주 사용되지 않는 단어나 영어가 아닌 단어들은 <unk>으로 이미 전처리가 되어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKf8zNuISiC2"
      },
      "source": [
        "import urllib\n",
        "with urllib.request.urlopen('https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt') as f:\n",
        "    data = f.readlines()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBLNOlRKSpOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a03271-c440-48f1-c6e3-4fb967ea17cf"
      },
      "source": [
        "print('num_sentence:',len(data))\n",
        "data[100]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_sentence: 42068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\" plans that give advertisers discounts for maintaining or increasing ad spending have become permanent <unk> at the news <unk> and underscore the fierce competition between newsweek time warner inc. 's time magazine and <unk> b. <unk> 's u.s. news & world report \\n\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLTv1EPbSwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "461a6247-6421-4fae-fa79-f223a0c5939c"
      },
      "source": [
        "seq_length_list = []\n",
        "for line in data:\n",
        "  # 각 라인별 단어 개수\n",
        "    seq_length_list.append(len(line.split()))\n",
        "\n",
        "counts, bins = np.histogram(seq_length_list, bins=20)\n",
        "plt.hist(bins[:-1], bins, weights=counts)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SdattmOcRwC"
      },
      "source": [
        "데이터에 있는 문장 길이들의 histogram을 볼 때 대부분의 data의 문장 길이가 50에 미치지 못하기 때문에 \\\\\n",
        "model에 집어넣을 최대 문장 길이를 50으로 세팅해두도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7MuFqsKcd4U"
      },
      "source": [
        "max_seq_len = 50"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyMpsyX8TwYy"
      },
      "source": [
        "### Build Dictionary\n",
        "\n",
        "먼저 text 데이터를 모델에 넣어주기 위해서는 text에 존재하는 단어들을 index로 변환해주어야 합니다.\n",
        "\n",
        "이를 위해서는 단어를 index로 변환해주는 word2idx dictionary와 다시 index를 단어로 변환해주는 idx2word dictionary를 만들어야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZmyZhcpTvZz"
      },
      "source": [
        "# UNK - \"unknown token\" - is used to replace the rare words that did not fit in your vocabulary.\n",
        "# GO - the same as <start> on the picture below - the first token which is fed to the decoder along with the though vector in order to start generating tokens of the answer\n",
        "# EOS - \"end of sentence\" - the same as <end> on the picture below - as soon as decoder generates this token we consider the answer to be complete (you can't use usual punctuation marks for this purpose cause their meaning can be different)\n",
        "# UNK - \"unknown token\" - is used to replace the rare words that did not fit in your vocabulary. So your sentence My name is guotong1988 will be translated into My name is _unk_.\n",
        "# PAD - your GPU (or CPU at worst) processes your training data in batches and all the sequences in your batch should have the same length. If the max length of your sequence is 8, your sentence My name is guotong1988 will be padded from either side to fit this length: My name is guotong1988 _pad_ _pad_ _pad_ _pad_\n",
        "def build_dictionary(data, max_seq_len):\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    ## Build Dictionary\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "    idx2word[0] = '<pad>'\n",
        "    idx2word[1] = '<unk>'\n",
        "    idx = 2\n",
        "    for line in data:\n",
        "        words = line.decode('utf-8').split()\n",
        "        words = words[:max_seq_len]        \n",
        "        ### Build Dictionary to convert word to index and index to word\n",
        "        ### YOUR CODE HERE (~ 5 lines)\n",
        "        for word in words:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = idx\n",
        "                idx2word[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = build_dictionary(data, max_seq_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPfV0OTc4Xdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d34bf5d-12a6-4ee0-db91-17863247c09f"
      },
      "source": [
        "if len(word2idx) == len(idx2word) == 10000:\n",
        "    print(\"Test Passed!\")\n",
        "else:\n",
        "    raise AssertionError"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me_m8njoXHrv"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "이제 앞서 만든 dictionary를 이용해서 text로된 데이터셋을 index들로 변환시키겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6fuARgzXEDU"
      },
      "source": [
        "def preprocess(data, word2idx, idx2word, max_seq_len):\n",
        "    tokens = []\n",
        "    for line in data:\n",
        "        words = line.decode('utf-8').split()\n",
        "        words = words[:max_seq_len]\n",
        "        ### Convert dataset with tokens\n",
        "        ### For each line, append <pad> token to match the number of max_seq_len\n",
        "        ### YOUR CODE HERE (~ 4 lines)\n",
        "        words += ['<pad>']*(max_seq_len - len(words))\n",
        "        for word in words:\n",
        "            token = word2idx[word]\n",
        "            tokens.append(token)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(data, word2idx, idx2word, max_seq_len)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjyvqMgbZnfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2548c751-3b69-4f9d-e007-9db6d58f37b1"
      },
      "source": [
        "if len(tokens) == 2103400:\n",
        "    print(\"Test Passed!\")\n",
        "else:\n",
        "    raise AssertionError"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmQxX3BH-SAv"
      },
      "source": [
        "이제 전처리된 Token들을 문장 단위의 배열로 변환시켜 두겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMvtp23-Jye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82ee6f4-328d-442d-e3e9-e4c4ec1449b4"
      },
      "source": [
        "tokens = np.array(tokens).reshape(-1, max_seq_len)\n",
        "print(tokens.shape)\n",
        "tokens[100]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42068, 50)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([745,  93, 746, 739, 747, 181, 748, 467, 749, 740, 750, 154, 751,\n",
              "       752,   1, 160,  32, 753,   1,  48, 754,  32, 755, 756, 757, 728,\n",
              "       555, 758,  99, 119, 555, 733,  48,   1, 759,   1, 119, 237, 753,\n",
              "       230, 760, 347,   0,   0,   0,   0,   0,   0,   0,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pceBqmtTZ9g9"
      },
      "source": [
        "### DataLoader\n",
        "\n",
        "이제 전처리된 dataset을 활용하여 PyTorch style의 dataset과 dataloader를 만들도록 하겠습니다.\n",
        "\n",
        "Token형태의 데이터를 PyTorch 스타일의 dataset으로 만들 때 주의할 점은, 추후 embedding matrix에서 indexing을 해주기 위해서 각 token이 LongTensor 형태로 정의되어야 한다는 점입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hAwhG1K9iBI"
      },
      "source": [
        "class LMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokens):\n",
        "        super(LMDataset, self).__init__()\n",
        "        self.PAD = 0\n",
        "        self.UNK = 1\n",
        "        self.tokens = tokens\n",
        "        self._getitem(2)\n",
        "\n",
        "    def _getitem(self, index):\n",
        "        X = self.tokens[index]\n",
        "        y = np.concatenate((X[1:], [self.PAD]))\n",
        "\n",
        "        X = torch.from_numpy(X).unsqueeze(0).long()\n",
        "        y = torch.from_numpy(y).unsqueeze(0).long()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.tokens[index]\n",
        "        y = np.concatenate((X[1:], [self.PAD]))\n",
        "\n",
        "        X = torch.from_numpy(X).long()\n",
        "        y = torch.from_numpy(y).long()\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiLNqM6kAda1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c41932a-67b2-436f-8077-fc732f8be1ea"
      },
      "source": [
        "batch_size = 64\n",
        "dataset = LMDataset(tokens)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(len(dataset))\n",
        "print(len(dataloader))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42068\n",
            "658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1nhBnqWxw4a"
      },
      "source": [
        "# 2. Model\n",
        "\n",
        "이번 section에서는 Language Modeling을 위한 Recurrent Model을 직접 만들어보도록 하겠습니다.\n",
        "\n",
        "Standard한 Recurrent Neural Network (RNN) model은 vanishing gradient 문제에 취약하기 때문에, 이번 실습에서는 변형된 RNN구조인 LSTM model을 활용하도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoNVt3MDOjl"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lycT_9vwaJN"
      },
      "source": [
        "LSTM model의 전체적인 구조와 각 gate의 수식은 아래와 같습니다.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1n93tpNW55Xl4GxZNcJcbUVRhuNCGH38h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1h6nfvYwN8n"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1nH9U5iD9cO6OVVTbrx-LjypRvcWzbOCU)\n",
        "\n",
        "LSTM의 자세한 동작방식이 궁금하신 분은 아래의 블로그를 참조해주세요.\n",
        "\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDNAysVqxxOk"
      },
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        # input-gate\n",
        "        self.Wi = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # forget-gate\n",
        "        self.Wf = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # gate-gate\n",
        "        self.Wg = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # output-gate\n",
        "        self.Wo = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, h_0, c_0):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            input (x): [batch_size, input_size]\n",
        "            hidden_state (h_0): [batch_size, hidden_size]\n",
        "            cell_state (c_0): [batch_size, hidden_size]\n",
        "        Outputs\n",
        "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
        "            next_cell_state (c_1): [batch_size, hidden_size]    \n",
        "        \"\"\"\n",
        "        h_1, c_1 = None, None\n",
        "        input = torch.cat((x, h_0), 1)\n",
        "        # Implement LSTM cell as noted above\n",
        "        ### YOUR CODE HERE (~ 6 lines)\n",
        "        i = self.sigmoid(self.Wi(input))\n",
        "        f = self.sigmoid(self.Wf(input))\n",
        "        g = self.tanh(self.Wg(input))\n",
        "        o = self.sigmoid(self.Wo(input))\n",
        "        c_1 = f * c_0 + i * g\n",
        "        h_1 = o * self.tanh(c_1)\n",
        "\n",
        "        return h_1, c_1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Tff2VCJ56D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b33ac24-19dd-44e7-86d1-87fc643bcf73"
      },
      "source": [
        "def test_lstm():\n",
        "    batch_size = 2\n",
        "    input_size = 5\n",
        "    hidden_size = 3\n",
        "\n",
        "    #torch.manual_seed(1234)\n",
        "    lstm = LSTMCell(input_size ,hidden_size)\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.constant_(m.weight, 0.1)\n",
        "            m.bias.data.fill_(0.01)\n",
        "    lstm.apply(init_weights)\n",
        "\n",
        "    x = torch.ones(batch_size, input_size)\n",
        "    hx = torch.zeros(batch_size, hidden_size)\n",
        "    cx = torch.zeros(batch_size, hidden_size)\n",
        "\n",
        "    hx, cx = lstm(x, hx, cx)\n",
        "    assert hx.detach().allclose(torch.tensor([[0.1784, 0.1784, 0.1784], \n",
        "                                              [0.1784, 0.1784, 0.1784]]), atol=2e-1), \\\n",
        "            f\"Output of the hidden state does not match.\"\n",
        "    assert cx.detach().allclose(torch.tensor([[0.2936, 0.2936, 0.2936], \n",
        "                                              [0.2936, 0.2936, 0.2936]]), atol=2e-1), \\\n",
        "            f\"Output of the cell state does not match.\"\n",
        "\n",
        "    print(\"==LSTM cell test passed!==\")\n",
        "\n",
        "test_lstm()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==LSTM cell test passed!==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DxU-78B33dG"
      },
      "source": [
        "## Language Model\n",
        "\n",
        "이제, 위에서 정의한 LSTM Cell을 활용해서 아래와 같은 Langauge Model을 만들어보도록 하겠습니다.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nMAbL-g31nERM44dgohA3k9Vj_92hIh-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0U2s0hux_n6"
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=64, vocab_size=10000):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        \n",
        "        self.input_layer = nn.Embedding(vocab_size, input_size)\n",
        "        self.hidden_layer = LSTMCell(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, hx, cx, predict=False):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            input (x): [batch_size]\n",
        "            hidden_state (h_0): [batch_size, hidden_size]\n",
        "            cell_state (c_0): [batch_size, hidden_size]\n",
        "            predict: whether to predict and sample the next word\n",
        "        Outputs\n",
        "            output (ox): [batch_size, hidden_size]\n",
        "            next_hidden_state (h_1): [batch_size, hidden_size]\n",
        "            next_cell_state (c_1): [batch_size, hidden_size]    \n",
        "        \"\"\"\n",
        "        x = self.input_layer(x)\n",
        "        hx, cx = self.hidden_layer(x, hx, cx)\n",
        "        ox = self.output_layer(hx)\n",
        "\n",
        "        if predict == True:\n",
        "            probs = F.softmax(ox, dim=1)\n",
        "            # torch distribution allows sampling operation\n",
        "            # see https://pytorch.org/docs/stable/distributions.html\n",
        "            dist = torch.distributions.Categorical(probs)\n",
        "            ox = dist.sample()\n",
        "\n",
        "        return ox, hx, cx  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ZpuMhsbBS8"
      },
      "source": [
        "# 3. Trainer\n",
        "\n",
        "자 이제 위에서 구현한 dataloader와 langauge model을 활용해서 모델의 학습을 진행해보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7TY7HmvbRlB"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, \n",
        "                 word2idx, \n",
        "                 idx2word,\n",
        "                 dataloader, \n",
        "                 model, \n",
        "                 criterion,\n",
        "                 optimizer, \n",
        "                 device):\n",
        "        \"\"\"\n",
        "        dataloader: dataloader\n",
        "        model: langauge model\n",
        "        criterion: loss function to evaluate the model (e.g., BCE Loss)\n",
        "        optimizer: optimizer for model\n",
        "        \"\"\"\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "        self.dataloader = dataloader\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        \n",
        "    def train(self, epochs = 1):\n",
        "        self.model.to(self.device)\n",
        "        start_time = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            for iter, (x_batch, y_batch) in tqdm.tqdm(enumerate(self.dataloader)):\n",
        "                self.model.train()\n",
        "                \n",
        "                batch_size, max_seq_len = x_batch.shape\n",
        "                x_batch = x_batch.to(self.device)\n",
        "                y_batch = y_batch.to(self.device)\n",
        "\n",
        "                # initial hidden-states\n",
        "                hx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
        "                cx = torch.zeros(batch_size, hidden_size).to(self.device)\n",
        "\n",
        "                # Implement LSTM operation\n",
        "                ox_batch = []\n",
        "                # Get output logits for each time sequence and append to the list, ox_batch\n",
        "                # YOUR CODE HERE (~ 4 lines)\n",
        "                for s_idx in range(max_seq_len):\n",
        "                    x = x_batch[:, s_idx]\n",
        "                    ox, hx, cx = self.model(x, hx, cx)\n",
        "                    ox_batch.append(ox)\n",
        "                # outputs are ordered by the time sequence\n",
        "                ox_batch = torch.cat(ox_batch).reshape(max_seq_len, batch_size, -1)\n",
        "                ox_batch = ox_batch.permute(1,0,2).reshape(batch_size*max_seq_len, -1)\n",
        "                y_batch = y_batch.reshape(-1)\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                loss = self.criterion(ox_batch, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            end_time = time.time() - start_time\n",
        "            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n",
        "            print('Time [%s], Epoch [%d/%d], loss: %.4f'\n",
        "                  % (end_time, epoch+1, epochs, np.mean(losses)))\n",
        "            if epoch % 5 == 0:\n",
        "                generated_sentences = self.test()\n",
        "                print('[Generated Sentences]')\n",
        "                for sentence in generated_sentences:\n",
        "                    print(sentence)\n",
        "            \n",
        "    def test(self):\n",
        "        # Test model to genereate the sentences\n",
        "        self.model.eval()\n",
        "        num_sentence = 5\n",
        "        max_seq_len = 50\n",
        "\n",
        "        # initial hidden-states\n",
        "        outs = []\n",
        "        x = torch.randint(0, 10000, (num_sentence,)).to(self.device)\n",
        "        hx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
        "        cx = torch.zeros(num_sentence, hidden_size).to(self.device)\n",
        "\n",
        "        outs.append(x)\n",
        "        with torch.no_grad():\n",
        "            for s_idx in range(max_seq_len-1):\n",
        "                x, hx, cx = self.model(x, hx, cx, predict=True)\n",
        "                outs.append(x)\n",
        "        outs = torch.cat(outs).reshape(max_seq_len, num_sentence)\n",
        "        outs = outs.permute(1, 0)\n",
        "        outs = outs.detach().cpu().numpy()\n",
        "\n",
        "        sentences = []\n",
        "        for out in outs:\n",
        "            sentence = []\n",
        "            for token_idx in out:\n",
        "                word = self.idx2word[token_idx]\n",
        "                sentence.append(word)\n",
        "            sentences.append(sentence)\n",
        "       \n",
        "        return sentences"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1EE9KDvyeeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73896db1-d4ec-49df-8858-a96863d35949"
      },
      "source": [
        "for iter,(x_batch,y_batch) in (enumerate(dataloader)):\n",
        "    print(x_batch.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([64, 50])\n",
            "torch.Size([20, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgEJv1vWqNkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693fcfda-a9b4-4144-955c-0dfba9b13634"
      },
      "source": [
        "lr = 1e-2\n",
        "input_size = 128\n",
        "hidden_size = 128\n",
        "batch_size = 256\n",
        "\n",
        "dataset = LMDataset(tokens)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "model = LanguageModel(input_size=input_size, hidden_size=hidden_size)\n",
        "# NOTE: you should use ignore_index to ignore the loss from predicting the <PAD> token\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "trainer = Trainer(word2idx = word2idx,\n",
        "                  idx2word = idx2word,\n",
        "                  dataloader=dataloader, \n",
        "                  model = model,\n",
        "                  criterion=criterion,\n",
        "                  optimizer = optimizer,\n",
        "                  device=device)\n",
        "\n",
        "trainer.train(epochs=50)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:19,  8.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:00:19], Epoch [1/50], loss: 6.0588\n",
            "[Generated Sentences]\n",
            "['police', 'closed', 'selected', 'due', 'the', 'months', 'payment', 'over', 'that', 'the', 'budget', '<unk>', 'cleveland', 'that', 'he', 'succeeds', 'at', '<unk>', 'that', 'there', 'are', 'subject', 'to', 'recognize', 'traded', 'if', 'will', 'be', 'able', 'to', 'come', 'it', 'does', \"n't\", 'have', 'a', 'smart', 'stores', 'will', 'arranged', 'to', 'aid', 'by', 'hand', \"'s\", 'program-trading', 'having', 'attracting', 'points', 'order']\n",
            "['connolly', 'one', 'is', 'stable', 'designed', 'how', 'much', 'of', 'trespass', 'support', 'of', 'directors', 'crops', 'signal', 'the', 'young', 'work', 'in', 'the', 'federal', 'commission', 'bought', 'shows', 'in', 'fact', 'is', 'not', 'margins', 'as', 'well', 'as', 'a', 'round', 'of', 'program', 'plans', 'to', 'be', 'cited', 'when', 'they', 'to', 'make', 'those', 'for', '<unk>', '<unk>', 'and', 'the', 'state']\n",
            "['circulating', 'function', 'often', 'use', 'the', 'unusually', 'bid', 'earlier', 'a', 'car', 'of', 'their', 'books', 'in', 'stocks', 'a', 'bottle', 'custom', 'to', 'get', 'investment', 'garage', 'and', 'its', 'shares', 'turned', 'by', 'renewed', 'population', 'and', 'aerospace', 'realities', 'convertible', 'retirement', 'transaction', \"'s\", '<unk>', 'factory', 'rate', 'of', 'the', 'manner', 'group', 'of', '<unk>', 'hall', 'ms.', 'his', 'accumulation', 'home']\n",
            "['yetnikoff', 'cap', 'at', 'kidder', 'peabody', 'which', 'asked', 'were', 'easy', 'filed', 'as', 'revamped', 'related', 'to', 'yield', 'N', 'bay-area', 'would', 'be', 'no', 'east', 'as', 'a', 'note', 'of', '<unk>', 'run', 'such', 'next', 'week', 'conversation', 'and', 'mexico', 'new', 'york', 'such', 'interview', 'insurance', '<unk>', '<unk>', 'state', 'an', '<unk>', 'intervention', 'is', \"n't\", 'done', 'a', 'managing', 'acquisition']\n",
            "['preclude', 'trillion', 'consumer', 'held', '<unk>', 'at', 'the', 'transportation', 'index', 'posted', 'N', 'N', 'in', 'its', 'quarter', 'compared', 'and', 'making', 'big', 'board', \"'\", 'pact', 'approaches', 'use', 'into', 'rally', \"'s\", 'reported', 'a', 'N', 'points', 'higher', 'of', 'N', 'N', 'shares', 'of', '$', 'N', 'million', 'shares', 'over', 'japan', \"'s\", 'a', 'around', 'N', 'N', 'increase', 'the']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:00:34], Epoch [2/50], loss: 5.1921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:00:50], Epoch [3/50], loss: 4.8769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:06], Epoch [4/50], loss: 4.6698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:22], Epoch [5/50], loss: 4.5140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:38], Epoch [6/50], loss: 4.3870\n",
            "[Generated Sentences]\n",
            "['strengthening', 'support', 'formation', 'of', 'disorders', 'buying', 'interest', 'rates', 'may', 'auction', 'against', 'black', 'space', 'may', 'mean', 'the', '<unk>', 'norton', 'sells', 'advertising', 'soon', 'i', \"'ve\", 'got', 'a', 'political', 'word', 'when', 'his', 'dinner', 'there', \"'ll\", 'be', 'able', 'to', 'suppliers', 'planning', 'remain', 'one', 'dealer', 'for', 'the', 'pentagon', \"'s\", 'obvious', 'to', 'sdi', 'in', 'the', 'recapitalization']\n",
            "['kemper', '<unk>', 'the', 'executive', 'through', 'an', 'additional', '$', 'N', 'million', 'account', 'for', 'making', '<unk>', 'estimated', 'of', 'N', 'billion', 'francs', '$', 'N', 'million', 'a', 'year', 'earlier', 'this', 'year', 'it', 'has', 'an', 'earlier', 'million', 'buy-out', 'of', 'the', 'year', 'if', 'make', 'the', 'big', 'board', 'of', 'united', 'airlines', 'carried', 'out', 'a', '<unk>', 'study', 'to']\n",
            "['erosion', 'bid', 'option', 'a', 'regular', '<unk>', 'operation', 'of', 'british', 'petroleum', 'pits', 'corp.', 'provisional', 'for', 'example', 'west', 'germany', 'said', 'that', 'if', 'the', 'price', 'white', 'knight', 'could', 'be', '<unk>', 'back', 'from', 'inception', 'in', 'cities', 'during', 'a', 'third', 'of', 'poland', \"'s\", 'insurance', 'controls', 'said', 'it', 'will', 'have', 'to', 'attract', 'pharmaceuticals', 'subsidiary', 'to', 'executives']\n",
            "['presidential', 'pont', 'also', 'were', 'active', 'for', 'a', 'period', 'a', 'principal', 'and', 'to', 'increase', 'equal', 'to', 'midland', 'the', '<unk>', 'they', 'wo', \"n't\", 'become', 'in', 'colombia', 'they', 'were', 'available', 'to', 'own', 'or', '<unk>', 'mr.', '<unk>', 'iowa', 'and', 'president', 'noriega', 'since', 'his', 'own', 'scenario', 'still', 'have', 'only', 'to', 'escape', 'prices', 'on', 'the', 'merc']\n",
            "['arab', 'tourists', 'occupied', 'the', 'sale', 'moreover', 'the', 'economy', 'series', 'N', 'years', 'were', 'due', 'N', 'and', 'said', 'it', 'is', 'to', '<unk>', 'the', 'record', 'a', '<unk>', 'seven', 'million', 'was', 'designed', 'to', 'victims', '<unk>', 'roy', 'produced', '<unk>', 'and', 'people', 'who', 'have', 'withheld', 'only', 'a', 'number', 'to', 'win', 'conventional', 'development', 'sensitive', 'to', 'make', 'their']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:01:54], Epoch [7/50], loss: 4.2789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:10], Epoch [8/50], loss: 4.1856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:26], Epoch [9/50], loss: 4.1063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:42], Epoch [10/50], loss: 4.0349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:02:58], Epoch [11/50], loss: 3.9703\n",
            "[Generated Sentences]\n",
            "['dutch', 'auction', 'including', 'the', 'idea', 'about', 'my', 'apple', '<unk>', 'the', 'polish', 'government', 'which', 'occurred', 'in', 'court', '<unk>', 'old', 'adding', 'to', '<unk>', 'the', 'task', 'of', 'paper', 'it', 'a', 'first', 'violation', 'he', 'says', 'in', 'for', 'instance', 'that', 'expected', 'typical', 'of', 'china', \"'s\", 'balance', 'fourth-quarter', 'said', 'on', 'earth', \"'s\", 'cost', 'overruns', 'on', 'a']\n",
            "['kan.', 'benefits', 'or', '<unk>', 'N', 'N', 'of', 'it', 'has', 'too', 'big', 'duty', '#', 'N', 'million', 'a', 'year', 'earlier', 'according', 'to', 'the', 'forecasting', 'scheme', 'or', 'by', 'soviet', 'economic', 'fundamentals', 'since', 'the', 'second', 'copy', 'of', 'chromosome', 'N', 'also', 'a', 'stress-related', 'illness', 'in', 'the', 'mood', 'are', 'also', 'an', '<unk>', 'of', 'political', 'conservatives', 'denounced']\n",
            "['floors', 'N', 'miles', 'from', 'the', 'country', \"'s\", 'worker', '<unk>', 'soviet', 'ice', 'samples', 'are', \"n't\", 'alone', 'and', 'you', 'do', \"n't\", 'clear', 'the', 'cost', 'package', 'a', '<unk>', 'said', 'frank', '<unk>', 'the', 'cambria', \"'s\", 'proposal', 'says', 'gerald', '<unk>', 'a', 'palestinian', 'agent', 'rule', 'said', 'the', 'chairman', 'and', 'chief', 'executive', 'vice', 'president', 'of', 'holding', 'co.']\n",
            "['drifted', 'overcapacity', 'started', 'a', 'suit', 'regarding', 'the', 'board', 'to', 'a', 'proposition', 'on', 'junk', 'mail', 'concrete', 'students', 'looking', 'for', 'colon', 'cancer', 'in', 'which', 'mr.', 'noriega', 'is', 'intentions', 'to', 'put', 'him', 'for', 'his', 'support', 'to', 'ask', 'see', 'apple', 'and', 'a', '<unk>', 'to', 'the', 'marlowe', '<unk>', 'the', 'former', 'president', \"'s\", '<unk>', 'letter', 'for']\n",
            "['maybe', 'the', 'world', 'supports', 'or', 'staff', 'group', 'congress', 'sought', 'for', 'everyone', 'is', 'likely', 'to', 'be', 'a', 'takeover', 'attempt', 'i', 'could', 'be', 'deprived', 'any', 'potentially', 'N', 'million', 'jobs', 'to', '$', 'N', 'billion', 'into', 'N', 'million', 'francs', '$', 'N', 'billion', 'in', 'maturing', 'bills', 'for', 'gm', 'and', 'introduced', 'more', 'than', '$', 'N', 'million']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:03:14], Epoch [12/50], loss: 3.9144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:03:30], Epoch [13/50], loss: 3.8632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:03:46], Epoch [14/50], loss: 3.8165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:02], Epoch [15/50], loss: 3.7725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:18], Epoch [16/50], loss: 3.7355\n",
            "[Generated Sentences]\n",
            "['prominent', 'carl', 'b.', '<unk>', 'director', 'of', 'the', '<unk>', 'discussions', 'on', 'behalf', 'of', 'the', 'proceeds', 'to', 'allow', 'compaq', 'and', 'after', 'trading', 'including', 'planned', 'stock', 'demand', 'and', 'bond', '<unk>', 's.a.', 'a', 'total', 'research', 'program', 'available', 'to', 'more', 'than', 'N', 'units', 'last', 'week', 'when', 'european', 'airlines', 'swung', 'to', 'double', 'by', 'the', 'accounting', 'concern']\n",
            "['again', 'kept', 'slightly', 'worse', 'than', 'the', 'words', 'on', 'the', 'matter', 'mr.', 'hahn', 'showed', 'a', '<unk>', 'of', 'the', 'federal', 'bureau', \"'s\", 'ties', 'to', 'public', 'cold', 'fusion', 'experiments', 'mr.', '<unk>', 'says', 'his', 'N', 'regular-season', '<unk>', 'is', 'simply', '<unk>', 'up', 'from', 'the', 'state', 'charges', 'military', 'chips', 'that', 'manager', 'for', 'most', 'of', 'theirs', 'in']\n",
            "['credits', 'similarly', 'N', 'N', 'in', 'march', 'N', 'telephone', 'usage', 'in', 'N', 'has', 'been', 'given', 'a', 'devices', 'valued', 'at', 'a', 'N', 'N', 'N', 'equity', 'stake', 'in', 'el', 'monte', 'unit', 'of', 'general', 'electric', 'co.', 'boston', 'co.', 'des', 'an', 'accident', 'at', 'which', 'six', 'months', 'coca-cola', 'co.', 'valued', 'at', '$', 'N', 'million', 'or', 'six']\n",
            "['globe', 'columnist', 'mike', 'a', 'full-time', 'administrator', 'when', 'angered', 'to', 'have', 'each', 'other', 'word', 'would', 'no', 'longer', 'carry', 'azt', 'to', 'reduce', 'its', 'retailing', 'empire', 'in', 'the', 'law', 'who', 'also', 'saw', 'the', 'confederation', 'of', 'overseas', 'and', 'other', 'systems', 'industrial', 'services', 'and', 'canada', \"'s\", 'budget', 'director', 'and', 'kodak', 'securities', 'that', 'can', '<unk>', 'each']\n",
            "['once', 'few', 'lawmakers', 'supervisors', 'energy', 'were', 'less', 'than', 'come', 'in', 'high-risk', 'germany', 'or', '<unk>', 'in', 'the', 'past', 'five', 'men', 'and', 'women', 'he', 'is', 'no', 'matter', 'if', 'we', 'have', 'made', 'to', 'exist', 'until', 'it', 'places', 'in', 'green', 'business', 'magazines', 'and', 'brick', 'wall', 'set', 'of', 'everyday', 'blue', 'jeans', 'a', 'truce', 'only', 'one']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:15, 10.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:34], Epoch [17/50], loss: 3.6988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:04:50], Epoch [18/50], loss: 3.6677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:06], Epoch [19/50], loss: 3.6373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:18,  9.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:24], Epoch [20/50], loss: 3.6075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:17,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:42], Epoch [21/50], loss: 3.5814\n",
            "[Generated Sentences]\n",
            "['genetically', 'both', 'british', 'capital', 'corp.', 'began', 'offering', 'certificates', 'of', 'intelogic', 'corp.', \"'s\", 'fund', 'and', 'the', 'advance', 'from', 'the', 'sale', 'late', 'in', '<unk>', 'capital', 'corp.', 'by', 'foreigners', 'ended', 'with', 'N', 'and', 'N', 'even', 'though', 'half', 'the', 'annual', 'domestic', 'defaults', 'will', 'be', 'heavy', 'debt', 'by', 'secret', 'requirements', 'the', 'price', 'of', '$', 'N']\n",
            "['ethiopia', 'defaults', 'just', 'N', 'into', 'usair', 'products', 'as', 'stores', 'of', 'assets', 'such', 'as', 'timing', 'and', 'passed', 'subsidies', 'for', 'a', 'nation', '<unk>', 'to', 'put', 'the', '<unk>', 'of', 'the', 'semiconductor', 'technology', 'and', 'work', 'force', 'illegal', 'by', 'president', 'and', '<unk>', '<unk>', 'des', 'assurances', 'director', 'for', 'both', 'the', 'resort', 'and', 'mr.', 'stoll', 'agreed', 'to']\n",
            "['transfers', 'award', 'metal', 'fell', 'broadly', 'for', 'the', 'year', 'ended', 'at', 'N', 'up', 'N', 'south', 'america', 'a', 'N', 'N', 'interest', 'for', 'trade', 'and', 'N', 'N', 'third-quarter', 'earnings', 'in', 'august', 'the', 'dow', 'jones', 'industrial', 'average', 'rolled', 'out', 'lowered', 'its', 'losses', 'and', 'a', 'strike', 'to', 'the', 'port', 'of', 'phillips', 'petroleum', 'co.', 'which', 'expects']\n",
            "['possibly', 'span', 'identical', 'offered', 'recently', 'increased', 'investors', 'control', 'data', \"'s\", '<unk>', 'note', 'held', 'earlier', 'a', 'day', 'by', 'N', 'N', 'down', 'N', 'point', 'in', 'the', 'mmi', 'except', 'said', 'its', '<unk>', 'will', '<unk>', 'one', 'of', 'its', 'nose', 'for', 'distant', 'deal', 'with', 'the', 'group', 'montedison', 's.p', 'a.', 'which', 'is', 'creating', 'a', 'small', 'gain']\n",
            "['bursts', 'of', 'an', '<unk>', '<unk>', 'already', 'cash', 'with', '<unk>', 'and', '<unk>', 'belts', 'recently', 'involving', 'N', 'anthrax', '<unk>', 'N', 'responses', 'to', 'the', 'u.s.', 'government', 'attempted', 'to', 'retire', 'as', 'federal', 'insurers', 'filed', '<unk>', '<unk>', 'a', 'stable', '<unk>', 'rolls', 'under', 'protection', 'from', 'the', '<unk>', '<unk>', 'in', 'panama', 'schools', 'in', 'her', 'work', 'by', '<unk>']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:05:58], Epoch [22/50], loss: 3.5606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:14], Epoch [23/50], loss: 3.5390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:30], Epoch [24/50], loss: 3.5162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:06:46], Epoch [25/50], loss: 3.4982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16,  9.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:03], Epoch [26/50], loss: 3.4802\n",
            "[Generated Sentences]\n",
            "['reviewed', 'read', 'democratic', 'american', '<unk>', 'institute', 'of', '<unk>', 'hills', '<unk>', 'luis', 'a.', '<unk>', 'is', 'a', 'compromise', 'committee', 'of', 'the', 'carrier', 'and', 'professional', 'dave', 'united', 'international', 'group', 'deputy', 'chief', 'said', 'maurice', 'saatchi', 'chief', 'executive', 'will', 'take', 'over', 'its', 'other', 'deadline', 'for', 'poland', 'and', 'hungary', 'where', 'despite', 'loose', 'mr.', 'phelan', 'said', 'sales']\n",
            "['faith', 'on', 'speculation', 'that', 'the', 'overall', 'material', 'might', 'someday', 'see', 'more', 'like', 'serious', 'problems', 'but', 'says', 'you', 'want', 'to', 'close', 'it', 'because', 'no', 'one', 'would', 'be', 'found', 'to', 'see', 'tactical', 'volatility', 'as', 'in', 'the', 'reagan', 'administration', 'that', 'is', 'having', 'that', 'damaged', 'money', 'buying', 'and', '<unk>', 'systems', 'argued', 'that', 'the', 'piano']\n",
            "['fcc', 'lives', 'and', 'troubled', 'u', 's', 'west', 'income', 'will', 'increase', 'increased', 'sales', 'growth', 'receipts', 'according', 'to', 'general', 'unsecured', 'high-quality', 'divisions', 'in', 'particular', 'prices', 'leaped', 'at', 'night', 'with', 'suspended', 'investment', 'banks', 'on', 'such', 'a', 'first', 'quarter', 'as', 'takeover', 'younger', 'generation', 'unit', 'harold', 's.', '<unk>', 'of', 'television', 'southmark', 'ltd.', 'from', 'two', 'separate']\n",
            "['reset', 'approval', 'for', 'early', 'next', 'month', 'of', 'the', 'boston', 'unit', 'net', 'to', 'N', 'billion', 'yen', '$', 'N', 'million', 'on', 'sales', 'of', '$', 'N', 'million', 'said', 'net', 'fell', 'through', 'last', 'friday', 'in', 'london', \"'s\", 'close', 'to', 'a$', 'N', 'million', 'compared', 'with', 'N', 'yen', 'each', 'compared', 'with', 'N', 'people', 'with', 'N', 'to']\n",
            "['mill', 'structural', 'lee', 'iron', 'corp.', 'and', '<unk>', '<unk>', '<unk>', '&', '<unk>', 'co.', 'and', '<unk>', '<unk>', '<unk>', \"'s\", '<unk>', 'group', 'said', 'the', 'beer', 'marketers', 'who', 'exchange', 'aggressively', 'that', 'readers', 'would', 'be', 'given', 'against', 'much', 'of', 'the', 'place', 'to', 'become', '<unk>', 'from', 'it', 'also', 'understood', 'how', 'to', 'promote', 'the', 'british', 'partner', 'of']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:20], Epoch [27/50], loss: 3.4645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:36], Epoch [28/50], loss: 3.4487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:07:52], Epoch [29/50], loss: 3.4357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:08], Epoch [30/50], loss: 3.4207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:17,  9.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:26], Epoch [31/50], loss: 3.4084\n",
            "[Generated Sentences]\n",
            "['establishing', 'may', 'be', 'more', 'the', 'opportunity', 'to', 'regain', 'their', 'memories', 'the', 'baseball', 'master', 'level', 'once', 'tried', 'to', 'exercise', 'their', 'currencies', 'prior', 'to', 'the', 'elections', 'in', 'one', 'neat', 'package', 'and', 'has', 'no', 'plans', 'for', 'annual', 'income', 'or', 'thursday', 'mips', 'next', 'spring', 'be', 'lifted', 'tomorrow', 'at', 'N', 'N', 'in', 'april', 'N', 'the']\n",
            "['assistant', 'sports', 'minister', '<unk>', '<unk>', 'd.', 'lewis', 'say', 'boston', \"'s\", 'lead', 'food', 'changes', 'is', 'including', 'the', 'webster', 'decision', 'with', 'how', 'the', '<unk>', '<unk>', 'bike', 'frame', 'this', 'week', \"'s\", 'product', 'is', 'imminent', 'following', 'a', 'screen', 'software', 'relations', 'with', 'the', 'same', 'security', 'game', 'with', '<unk>', 'johns', 'hopkins', 'residents', 'responding', 'to', 'N', 'the']\n",
            "['satisfy', 'meanwhile', 'benefited', 'his', 'return', 'on', 'the', 'human', 'court', 'rejected', 'a', 'law', 'by', 'mr.', 'thurmond', 'who', 'claims', 'a', 'speaker', 'of', 'unconstitutional', 'conditions', 'and', 'it', \"'s\", 'too', 'early', 'to', 'get', 'nearly', 'the', 'market', 'and', 'interior', 'departments', 'have', 'affected', 'audiences', 'their', 'turf', 'for', 'the', 'protection', 'if', 'to', 'close', 'a', 'capital-gains', 'provision', 'in']\n",
            "['which', 'is', 'he', 'concerned', 'this', 'comes', 'in', 'congressional', 'opinion', 'new', 'jersey', 'gop', 'gov.', 'nelson', 'rockefeller', 'lake', 'city', 'mich.', 'consented', 'to', 'a', '<unk>', 'cap', 'and', 'run', '<unk>', 'flights', 'from', '<unk>', 'and', 'selling', 'all', 'stereo', '<unk>', 'N', 'through', '<unk>', 'city', 'and', '<unk>', 'mills', 'apparel', '&', 'to', 'submit', '<unk>', '<unk>', 'to', 'the', 'supreme']\n",
            "['b.', 'dan', 'computer', 'intel', 'corp.', 'could', 'chairman', 'robert', 'f.', 'angelo', 'formerly', 'phoenix', 'has', 'sold', 'N', 'million', 'to', 'a', 'N', 'million', 'last', 'N', 'N', 'auction', 'in', 'the', 'troubled', 'rules', 'he', 'added', 'to', 'nasdaq', 'tough', 'competition', 'from', 'las', 'vegas', 'if', 'cbs', 'has', 'about', 'N', 'million', 'tons', 'of', 'privately', 'inch', 'nov.', 'N', 'for']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:42], Epoch [32/50], loss: 3.3991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:08:58], Epoch [33/50], loss: 3.3869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:14], Epoch [34/50], loss: 3.3764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:30], Epoch [35/50], loss: 3.3651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:09:47], Epoch [36/50], loss: 3.3564\n",
            "[Generated Sentences]\n",
            "['famous', 'though', 'mr.', 'lang', 'took', 'to', 'give', 'wilmington', 'square', 'feet', 'sea', 'containers', \"'\", 'name', 'from', 'calls', 'for', '<unk>', 'associates', 'a', '<unk>', 'company', 'in', 'return', 'for', 'intent', 'and', 'they', 'got', 'under', 'pressure', 'to', 'jointly', 'each', 'program', 'rather', 'than', 'the', 'broader', 'movements', 'have', 'been', 'to', 'improve', 'his', 'company', \"'s\", 'economic', 'commitments', 'to']\n",
            "['paid', 'substantial', 'corporate', 'steelmakers', 'especially', 'while', 'the', 'branch', 'becomes', 'as', 'a', '<unk>', 'of', 'a', 'series', 'a', 'package', 'that', 'should', 'be', '<unk>', 'about', 'those', 'things', 'as', 'is', 'costing', 'maturity', 'abortions', 'despite', 'well', '<unk>', 'jaguar', 'officials', 'were', 'ready', 'to', 'collect', 'returns', 'by', '<unk>', 'commercial', 'investigations', 'and', '<unk>', 'with', 'a', 'comparable', 'figure', 'each']\n",
            "['beyond', 'that', 'the', 'senate', 'then', 'delivered', 'three', 'others', 'can', 'be', 'moving', 'a', 'pit', 'with', 'the', 'level', 'of', 'mr.', 'hunt', \"'s\", 'allegations', 'they', \"'d\", 'like', 'financier', 'alan', 'b.', 'helmsley', \"'s\", 'assistant', 'secretary', 'of', 'state', 'laws', 'and', 'team', 'rehabilitation', 'managers', 'who', 'should', 'they', 'read', 'against', 'mr.', 'dinkins', 'on', 'the', 'streets', 'of', 'fire']\n",
            "['home', 'spent', 'years', 'ago', 'mr.', 'jones', 'has', 'said', 'the', 'two', 'days', 'N', 'charges', 'of', 'maryland', 'and', 'in', '<unk>', '19th', 'and', 'that', 'point', 'to', 'prevent', 'it', 'as', 'shape', 'walter', 'yetnikoff', 'the', 'new', 'york', 'attorney', 'wanted', 'to', 'put', 'a', '<unk>', 'like', '<unk>', '<unk>', '<unk>', '<unk>', 'architecture', 'oct.', 'N', 'is', '<unk>', 'for', 'those']\n",
            "['desktop', 'N', 'researchers', 'police', 'the', 'khmer', 'rouge', 'and', 'directors', 'with', 'prospective', 'buyers', 'who', 'needs', 'any', 'way', 'to', 'bid', 'that', 'it', 'would', 'use', 'the', '<unk>', 'plan', 'which', 'would', 'tighten', 'requirements', 'back', 'in', 'no', 'appreciation', 'bonds', 'available', 'bonds', 'to', 'brokers', 'on', 'taxes', 'and', 'tax', 'evasion', 'instead', 'of', 'repairs', 'of', 'scientific', 'calculations', 'to']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:03], Epoch [37/50], loss: 3.3499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:19], Epoch [38/50], loss: 3.3423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:35], Epoch [39/50], loss: 3.3384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:10:52], Epoch [40/50], loss: 3.3285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:08], Epoch [41/50], loss: 3.3214\n",
            "[Generated Sentences]\n",
            "['scheduling', 'respectability', 'income', 'in', 'the', 'first', 'nine', 'months', 'net', 'was', 'c$', 'N', 'million', 'from', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'after', 'the', 'release', 'rate', 'of', '$', 'N', 'million', 'a', 'year', 'earlier', 'aided', 'by', 'an', 'reported', 'loss', 'of', '$', 'N', 'million', 'in', 'united', 'trade', 'figures', 'would', 'issue', 'clients', 'to']\n",
            "['bill', '<unk>', 'sr.', 'court', 'awarded', 'to', 'finance', \"'s\", 'corporate', '<unk>', 'including', 'realist', \"'s\", 'operating', 'division', 'of', '$', 'N', 'billion', 'had', 'net', 'income', 'of', '$', 'N', 'million', 'up', '$', 'N', 'a', 'share', 'to', 'N', 'cents', 'a', 'share', 'in', 'a', 'fiscal', 'N', 'most', 'august', 'N', 'postal', 'trade', 'news', 'created', 'by', 's&p', 'of']\n",
            "['provisional', 'lawrence', '<unk>', \"'s\", '<unk>', 'family', 'also', 'made', 'any', 'investment', 'fund', 'apparently', 'squeezed', 'in', 'membership', 'revenues', 'from', 'customers', \"'\", 'action', 'against', 'orders', 'for', 'the', 'insurance', 'company', 'and', 'the', 'bank', \"'s\", 'lower', 'performance', 'than', 'the', 'largest', 'economy', 'for', 'approximately', 'price', 'and', 'the', 'canadian', 'purchase', 'N', 'N', 'N', 'bonds', 'were', 'unchanged', 'at']\n",
            "['comprehensive', 'care', 'to', 'purchase', 'assets', 'last', 'year', 'would', 'have', 'been', 'difficult', 'to', 'enforce', 'market', 'investments', 'in', 'a', 'significant', 'reduction', 'in', 'the', 'property', 'increasing', 'rate', 'emissions', 'of', 'N', 'N', 'to', 'N', 'tons', 'according', 'to', 'officials', 'familiar', 'dance', 'even', 'other', 'curb', 'productivity', 'gains', 'slowed', 'to', 'rising', 'imports', 'as', 'a', 'result', 'of', 'its']\n",
            "['cosby', 'adrs', 'carat', 'models', 'disclosed', 'it', 'makes', 'imports', 'some', 'votes', 'reflect', '12-month', 'plus', 'accrued', 'interest', 'because', 'which', 'invest', 'in', 'performance', 'from', 'a', 'transaction', 'that', 'a', 'sizable', 'yield', 'figure', 'priced', 'at', 'a', 'yield', 'to', 'maturity', 'the', 'N', 'N', 'payable', 'date', 'and', 'N', 'on', 'the', '$', 'N', 'billion', 'buy-out', 'of', 'united', 'airlines']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:24], Epoch [42/50], loss: 3.3181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:40], Epoch [43/50], loss: 3.3107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:11:56], Epoch [44/50], loss: 3.3038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:12], Epoch [45/50], loss: 3.3013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:28], Epoch [46/50], loss: 3.2945\n",
            "[Generated Sentences]\n",
            "['guerrillas', 'already', 'own', 'any', 'combined', 'with', 'gains', 'in', 'the', 'fiscal', 'year', 'ending', 'the', '$', 'N', 'level', 'to', 'prove', 'the', 'uncertainty', 'as', 'a', 'cause', 'for', 'news', 'they', 'do', \"n't\", 'anticipate', 'a', 'hurry', 'for', 'any', 'detail', 'you', \"'d\", 'need', 'its', 'chance', 'to', 'integrate', 'their', 'hands', 'from', 'borrowers', 'have', 'means', 'behind', 'under', 'a']\n",
            "['players', \"'\", 'customers', 'can', '<unk>', 'this', 'has', 'slashed', 'ratings', 'on', 'u.s.', 'treasury', 'yields', 'last', 'year', 'and', 'replaced', 'by', 'a', 'wide', 'variety', 'of', 'schemes', 'than', 'they', 'should', 'pose', 'a', '<unk>', 'full', 'complaint', 'while', 'the', 'actual', '<unk>', 'can', 'exercise', 'people', 'of', 'setting', 'a', 'costly', 'bidding', 'featuring', 'rent', 'and', 'mortgage', 'markets', 'to', 'try']\n",
            "['beverage', 'alcohol', 'and', 'editors', 'are', 'being', 'settled', 'in', 'market', 'share', 'prices', 'in', 'one', 'difference', 'between', 'N', 'N', 'when', 'fell', 'N', 'N', 'this', 'year', 'because', 'investors', 'take', 'a', 'pull', 'ahead', 'of', 'soviet', 'payments', 'some', 'customers', 'and', 'businesses', 'in', 'the', 'wake', 'of', 'the', 'purchase', 'of', 'the', 'privately', 'two', 'days', 'he', 'realized', 'that']\n",
            "['keith', '<unk>', 'is', 'a', 'named', 'news', 'that', 'the', 'flight', 'would', 'be', 'settled', 'by', 'star', '<unk>', 'who', 'allegedly', 'commit', 'a', 'bid', 'an', 'attempt', 'to', 'sabotage', 'change', 'in', 'an', 'area', 'with', 'toyota', 'and', '<unk>', '<unk>', 'from', 'being', 'everywhere', 'before', 'it', 'has', 'less', 'much', 'closer', 'to', 'for', 'work', 'including', 'an', 'american', 'flag', 'while']\n",
            "['lineup', 'sends', 'throughout', 'throughout', 'the', 'nine', 'months', 'fourth', 'quarter', 'ended', 'sept.', 'N', 'the', 'ashland', 'ky.', '<unk>', 'the', 'company', \"'s\", 'auto', 'operations', 'offer', 'is', 'subject', 'to', 'approval', 'by', 'the', '<unk>', 'union', 'group', 'led', 'by', 'increasing', 'international', 'market', 'efficiency', 'which', 'manages', 'more', 'than', 'three', 'weeks', 'ago', 'when', 'it', 'decided', 'to', 'help', 'chemicals']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:12:44], Epoch [47/50], loss: 3.2897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:13:00], Epoch [48/50], loss: 3.2831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:13:16], Epoch [49/50], loss: 3.2795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "165it [00:16, 10.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time [0:13:32], Epoch [50/50], loss: 3.2790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDhlrcENM4Dx"
      },
      "source": [
        "생성된 텍스트의 퀄리티는 어떤가요? \n",
        "\n",
        "앞으로 딥러닝 강의가 끝나면 자연어처리 강좌에서 텍스트 처리에 적합한 전처리 과정, 모델구조들을 본격적으로 배우시게 될것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ua-_6W2a5Lt"
      },
      "source": [
        "# References\n",
        "\n",
        "1. https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "2. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model"
      ]
    }
  ]
}